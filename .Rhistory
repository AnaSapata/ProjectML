header = TRUE)
data <- readRDS('/home/anasapata/Personal/datasciencecoursera/Exploratory Data Analysis/exdata_data_NEI_data/summarySCC_PM25.rds')
head(data)
NEI <- readRDS('/home/anasapata/Personal/datasciencecoursera/Exploratory Data Analysis/exdata_data_NEI_data/summarySCC_PM25.rds')
SCC <- readRDS('/home/anasapata/Personal/datasciencecoursera/Exploratory Data Analysis/exdata_data_NEI_data/Source_Classification_Code.rds')
head(NEI)
head(SCC)
sapply(NEI$Pollutant, mean)
sapply(NEI$Emission, mean)
lapply(NEI$Emissions, mean)
total_per_year = NEI %>% group_by(year) %>% summarize(Emissions = sum(Emissions))
library(dplyr)
total_by_year <- NEI %>% group_by(year) %>% summarize(Emissions = sum(Emissions))
total_by_year
plot(total_by_year$year, total_by_year$Emissions)
plot(total_by_year$year, total_by_year$Emissions, xlab = "Year", ylab = "Emissions", main = "Total of emissions by year")
Maryland_data <- subset(NEI, NEI$fips=="24510")
Maryland_data_by_year <- Maryland_data %>% group_by(year) %>% summarize(Emissions = sum(Emissions))
plot(Maryland_data_by_year$year, Maryland_data_by_year$Emissions, xlab = "Year", ylab = "Emissions", main = "Total of emissions by year in Maryland")
Maryland_data_by_year
Maryland_data <- subset(NEI, NEI$fips=="24510" && (NEI$year == 1999 || NEI$year == 2008 ))
plot(Maryland_data$year, sum(Maryland_data$Emissions), xlab = "Year", ylab = "Emissions", main = "Total of emissions by year in Maryland")
Maryland_data_by_year <- Maryland_data %>% group_by(year) %>% summarize(Emissions = sum(Emissions))
plot(Maryland_data_by_year$year, Maryland_data_by_year$Emissions, xlab = "Year", ylab = "Emissions", main = "Total of emissions by year in Maryland")
Maryland_data_by_year
Maryland_data
Maryland_data <- subset(NEI, NEI$fips=="24510" & (NEI$year == 1999  NEI$year == 2008 ))
Maryland_data <- subset(NEI, NEI$fips=="24510" & (NEI$year == 1999 | NEI$year == 2008 ))
Maryland_data
Maryland_data_by_year <- Maryland_data %>% group_by(year) %>% summarize(Emissions = sum(Emissions))
plot(Maryland_data_by_year$year, Maryland_data_by_year$Emissions, xlab = "Year", ylab = "Emissions", main = "Total of emissions by year in Maryland")
Maryland_data_by_year
plot(Maryland_data$year, sum(Maryland_data$Emissions), xlab = "Year", ylab = "Emissions", main = "Total of emissions by year in Maryland")
Maryland_data
Maryland_data_by_year
plot(Maryland_data_by_year$year, Maryland_data_by_year$Emissions, xlab = "Year", ylab = "Emissions", main = "Total of emissions by year in Maryland")
setwd('/home/anasapata/Personal/datasciencecoursera/ML/Project')
setwd('/home/anasapata/Personal/datasciencecoursera/ML/Project')
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', 'training.csv')
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', 'testing.csv')
training <- read.csv('training.csv', header = TRUE)
testing <- read.csv('testing.csv', header = TRUE)
head(training)
head(testing)
dim(training)
dim(testing)
range(dim(training)[2])
range(1,dim(training)[2])
for (i in range(1,dim(training)[2])){
print i
}
for (i in range(1,dim(training)[2])){
print (i)
}
for (i in 1:(dim(training)[2])){
print (i)
}
for (i in 1:(dim(training)[2])){
sum(is.na(training[,i]))
}
for (i in 1:(dim(training)[2])){
print(sum(is.na(training[,i])))
}
colSums(is.na(training))
training <- training[, colSums(is.na(training))==0]
testing <- testing[, colSums(is.na(testing))==0]
dim(training)
dim(testing)
str(training)
str(testing)
training$X <- NULL
testing$X <- NULL
validation <- read.csv('testing.csv', header = TRUE)
validation$X <- NULL
head(validation)
dim(validation)
validation <- testing[, colSums(is.na(testing))==0]
validation <- validation[, colSums(is.na(testing))==0]
validation <- read.csv('testing.csv', header = TRUE)
validation$X <- NULL
head(validation)
dim(validation)
validation <- validation[, colSums(is.na(validation))==0]
rm (testing)
training <- read.csv('training.csv', header = TRUE)
validation <- read.csv('testing.csv', header = TRUE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
library(caret)
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
training <- createDataPartition(training$classe, p= 0.7, list = FALSE)
# Training and testing sets
training <- read.csv('training.csv', header = TRUE)
training$X <- NULL
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
# See the correlation between variables
cor_mat <- cor(training[, -53])
# See the correlation between variables
cor_mat <- cor(training[,])
training[,]
# See the correlation between variables
cor_mat <- cor(training[,-92])
# Training and testing sets
training <- read.csv('training.csv', header = TRUE)
validation <- read.csv('testing.csv', header = TRUE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,c(1:6)]
# Training and testing sets
training <- read.csv('training.csv', header = TRUE)
validation <- read.csv('testing.csv', header = TRUE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
testing <- testing[,-c(1:6)]
training <- read.csv('training.csv', header = TRUE)
validation <- read.csv('testing.csv', header = TRUE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
training[,]
# See the correlation between variables
cor_mat <- cor(training[,-86])
# See the correlation between variables
cor_mat <- cor(training[,-86])
str(training)
str(as.numeric(training))
as.numeric(training)
# Training and testing sets
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
str(training)
table(training$kurtosis_roll_belt)
install.packages("xts")
library(caret)
setwd('/home/anasapata/Personal/datasciencecoursera/ML/Project')
library(caret)
setwd('/home/anasapata/Personal/datasciencecoursera/ML/ProjectML')
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', 'training.csv')
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', 'testing.csv')
# Training and testing sets
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
# See the correlation between variables
cor_mat <- cor(training[,-86])
# See the correlation between variables
for (i in 1:dim(training)[2]){
training[,i] <- as.numeric(training[,i])
}
cor_mat <- cor(training[,-86])
for (i in 1:dim(testing)[2]){
testing[,i] <- as.numeric(testing[,i])
}
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
library(corrplot)
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
cor_mat
cor_mat <- cor(training[,-86], na.rm = TRUE)
colSums(is.na(training))
training$classe
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
training$kurtosis_roll_belt
training <- training[,-c(5,13)]
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
training <- training[,-c(5,13)]
validation <- validation[,-c(5:13)]
training <- training[,-c(5,13)]
validation <- validation[,-c(5:13)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
training <- training[,-c(5,13)]
training <- training[,-c(5,13)]
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
training <- training[,-c(5:13)]
validation <- validation[,-c(5:13)]
training <- training[,-c(27:32)]
training <- training[,-c(40:48)]
training <- training[,-c(30:38)]
# Training and testing sets
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
training <- training[,-c(5:13)]
training <- training[,-c(27:32)]
training <- training[,-c(30:38)]
training <- training[,-c(43:51)]
validation <- validation[,-c(27:32)]
validation <- validation[,-c(30:38)]
validation <- validation[,-c(43:51)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
cor_mat <- cor(training[,-86])
cor_mat <- cor(training[,-53])
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
corrplot(cor_mat, order = "FPC", method = "circle", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
corrplot(cor_mat, order = "FPC", method = "square", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
corrplot(cor_mat, order = "FPC", method = "shade", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
# See the correlation between variables
correlation <- cor(training[,-53])
corrplot(correlation, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
# Get the columns with high correlation(over 0.75)
high_cor = findCorrelation(correlation, cutoff=0.75)
# Training and testing sets
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
head(training)
head(validation)
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
# remove variables with values ""
training <- training[,-c(5:13)]
training <- training[,-c(27:32)]
training <- training[,-c(30:38)]
training <- training[,-c(43:51)]
validation <- validation[,-c(5:13)]
validation <- validation[,-c(27:32)]
validation <- validation[,-c(30:38)]
validation <- validation[,-c(43:51)]
# Split the training data between testing and training with 30%, 70% resp.
set.seed(2711)
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
training <- training[inTrain,]
testing <- training[-inTrain,]
# See the correlation between variables
correlation <- cor(training[,-53])
corrplot(correlation, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
# Get the columns with high correlation(over 0.75)
high_cor = findCorrelation(correlation, cutoff=0.75)
# Verify which are the columns with high correlation
names(training)[high_cor]
# 1st - Classification Trees
set.seed(2711)
decisionTreeMod1 <- rpart(classe ~ ., data=training, method="class")
library(rpart)
# 1st - Classification Trees
set.seed(2711)
decisionTreeMod1 <- rpart(classe ~ ., data=training, method="class")
DTFit1 <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(DTFit1)
library(rattle)
fancyRpartPlot(DTFit1)
png("Plot.png")
fancyRpartPlot(DTFit1)
dev.off()
jpeg("Plot.jpg")
fancyRpartPlot(DTFit1)
dev.off()
DTFit1
# Predictions
predictDTFit1 <- predict(DTFit1, testing, type = "class")
conf_mat_DTFit1 <- confusionMatrix(predictDTFit1, testing$classe)
conf_mat_DTFit1 <- confusionMatrix(table(predictDTFit1, testing$classe))
library(e1071)
conf_mat_DTFit1 <- confusionMatrix(table(predictDTFit1, testing$classe))
conf_mat_DTFit1
# 2nd - Random Forest
#controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFFit1 <- train(classe ~ ., data=training, method="rf")
# 2nd - Random Forest
#controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFFit1 <- train(classe ~ ., data=training, method="rf")
# 2nd - Random Forest
#controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFFit1 <- train(classe ~ ., data=training, method="rf")
# 2nd - Random Forest
#controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFFit1 <- train(classe ~ ., data=training, method="rf")
# 2nd - Random Forest
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFFit1 <- train(classe ~ ., data=training, method="rf", trControl = controlRF)
RFFit1$finalModel
# Predictions
predictRFFit1 <- predict(RFFit1, testing)
conf_mat_RFFit1 <- confusionMatrix(predictRFFit1, testing$classe)
conf_mat_RFFit1 <- confusionMatrix(table(predictRFFit1, testing$classe))
conf_mat_RFFit1
plot(RFFit1)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
GBMFit1 <- train( classe ~. , data = training, method = "gbm", trControl = controlGBM, verbose = FALSE)
GBMFit1$finalModel
print(GBMFit1)
# Predictions
predictGBMFit1 <- predict(GBMFit1, testing)
conf_mat_GBMFit1 <- confusionMatrix(predictGBMFit1, testing$classe)
conf_mat_GBMFit1 <- confusionMatrix(table(predictGBMFit1, testing$classe))
conf_mat_GBMFit1
conf_mat_DTFit1
conf_mat_RFFit1
# Accuracy =
conf_mat_GBMFit1
# The model with the high accuracy was RF, so i will apply it to the validation set
res <- predict(RFFit1, validation)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
validation$X <- NULL
validation <- validation[, colSums(is.na(validation))==0]
validation <- validation[,-c(1:6)]
# The model with the high accuracy was RF, so i will apply it to the validation set
res <- predict(RFFit1, validation)
# The model with the high accuracy was RF, so i will apply it to the validation set
res <- predict(RFFit1, validation); res
validation$classe
# The model with the high accuracy was RF, so i will apply it to the validation set
res <- predict(RFFit1, validation); res
plot(RFFit1)
prp(DTFit1)
library(rpart)
prp(DTFit1)
install.packages("rpart.plot")
library(rpart.plot)
prp(DTFit1)
fancyRpartPlot(DTFit1)
library(rattle)
)
fancyRpartPlot(DTFit1)
prp(DTFit1)
head(training)
dim(training)
dim(validation)
# Training and testing sets
training <- read.csv('training.csv', header = TRUE, stringsAsFactors = FALSE)
validation <- read.csv('testing.csv', header = TRUE, stringsAsFactors = FALSE)
training$X <- NULL
validation$X <- NULL
dim(training)
dim(validation)
# Remove the columns with NA's values
training <- training[, colSums(is.na(training))==0]
validation <- validation[, colSums(is.na(validation))==0]
# Remove first 6 variables
training <- training[,-c(1:6)]
validation <- validation[,-c(1:6)]
# remove variables with values ""
training <- training[,-c(5:13)]
training <- training[,-c(27:32)]
training <- training[,-c(30:38)]
training <- training[,-c(43:51)]
dim(training)
dim(validation)
# 1st - Decision Trees
set.seed(2711)
DTFit1 <- rpart(classe ~ ., data=training, method="class")
#fancyRpartPlot(DTFit1)
prp(DTFit1)
#fancyRpartPlot(DTFit1)
prp(DTFit1)
#fancyRpartPlot(DTFit1)
png('DTFit1.png')
prp(DTFit1)
dev.off()
# Predictions
predictDTFit1 <- predict(DTFit1, testing, type = "class")
conf_mat_DTFit1 <- confusionMatrix(table(predictDTFit1, testing$classe))
library(caret)
library(corrplot)
library(rpart)
library(rpart.plot)
library(rattle)
library(e1071)
conf_mat_DTFit1 <- confusionMatrix(table(predictDTFit1, testing$classe))
# Accuracy = .743
conf_mat_DTFit1
